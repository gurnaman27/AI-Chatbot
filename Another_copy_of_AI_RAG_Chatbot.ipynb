{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jthdNAr84ung",
        "outputId": "a0303f46-8cb3-4afc-b4a0-f4b19b856386",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/232.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n",
        "!pip install sentence-transformers\n",
        "!pip install langchain\n",
        "!pip install langchain_community\n",
        "!pip install PyPDF2 sentence-transformers langchain faiss-cpu tiktoken llama-cpp-python\n",
        "!pip install llama-cpp-python\n",
        "!pip install tiktoken\n",
        "!pip install gradio\n",
        "!pip install pickle5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRTqZidZfBak"
      },
      "outputs": [],
      "source": [
        "pwd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9qjCAoV-QwM"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "def download_model(url, destination_path):\n",
        "    # Stream the file from the URL in chunks\n",
        "    with requests.get(url, stream=True) as r:\n",
        "        r.raise_for_status()\n",
        "        with open(destination_path, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "\n",
        "# Example URL where the model file is hosted\n",
        "model_url = \"https://huggingface.co/TheBloke/stablelm-zephyr-3b-GGUF/resolve/main/stablelm-zephyr-3b.Q4_K_M.gguf?download=true\"\n",
        "\n",
        "# Destination path where the model file will be saved\n",
        "model_path = \"stablelm-zephyr-3b.Q4_K_M.gguf\"\n",
        "\n",
        "# Download the model file\n",
        "download_model(model_url, model_path)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q95PsxRs4s5w"
      },
      "outputs": [],
      "source": [
        "import PyPDF2\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.llms import LlamaCpp\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "import gradio as gr\n",
        "import pickle5 as pickle\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IUMHi0Oe-NRd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "801drkmF41v9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# pdf_docs=[\"Vinay Kumar Resume.pdf\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0i6jw0Y5F16"
      },
      "outputs": [],
      "source": [
        "\n",
        "def prepare_docs(pdf):\n",
        "    docs = []\n",
        "    metadata = []\n",
        "    content = []\n",
        "\n",
        "    # for pdf in pdf_docs:\n",
        "\n",
        "    pdf_reader = PyPDF2.PdfReader(pdf)\n",
        "    for index, text in enumerate(pdf_reader.pages):\n",
        "        doc_page = {'title': pdf + \" page \" + str(index + 1),\n",
        "                    'content': pdf_reader.pages[index].extract_text()}\n",
        "        docs.append(doc_page)\n",
        "    for doc in docs:\n",
        "        content.append(doc[\"content\"])\n",
        "        metadata.append({\n",
        "            \"title\": doc[\"title\"]\n",
        "        })\n",
        "    print(\"Content and metadata are extracted from the documents\")\n",
        "    return content, metadata\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23FsthKT5F7f"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "def get_text_chunks(content, metadata):\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "        chunk_size=512,\n",
        "        chunk_overlap=256,\n",
        "    )\n",
        "    split_docs = text_splitter.create_documents(content, metadatas=metadata)\n",
        "    print(f\"Documents are split into {len(split_docs)} passages\")\n",
        "    return split_docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0xdsLB85GAA"
      },
      "outputs": [],
      "source": [
        "def ingest_into_vectordb(split_docs):\n",
        "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "    db = FAISS.from_documents(split_docs, embeddings)\n",
        "\n",
        "    DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
        "    db.save_local(DB_FAISS_PATH)\n",
        "    return db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDhke0wc7xnq"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"[INST]\n",
        "As an AI, provide accurate and relevant information based on the provided document. Your responses should adhere to the following guidelines:\n",
        "- Answer the question based on the provided documents.\n",
        "- Be direct and factual, limited to 50 words and 2-3 sentences. Begin your response without using introductory phrases like yes, no etc.\n",
        "- Maintain an ethical and unbiased tone, avoiding harmful or offensive content.\n",
        "- If the document does not contain relevant information, state \"I cannot provide an answer based on the provided document.\"\n",
        "- Avoid using confirmatory phrases like \"Yes, you are correct\" or any similar validation in your responses.\n",
        "- Do not fabricate information or include questions in your responses.\n",
        "- do not prompt to select answers. do not ask me questions\n",
        "{question}\n",
        "[/INST]\n",
        "\"\"\"\n",
        "\n",
        "#template = \"\"\"Given the document and the current conversation between a user and an agent, your task is as follows: Answer any user query by using information from the document. The response should be detailed.\"\"\"\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "def get_conversation_chain(vectordb):\n",
        "    llama_llm = LlamaCpp(\n",
        "    model_path=\"stablelm-zephyr-3b.Q4_K_M.gguf\",\n",
        "    temperature=0.75,\n",
        "    max_tokens=200,\n",
        "    top_p=1,\n",
        "    callback_manager=callback_manager,\n",
        "    n_ctx=3000)\n",
        "\n",
        "    retriever = vectordb.as_retriever()\n",
        "    CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(template)\n",
        "\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key='chat_history', return_messages=True, output_key='answer')\n",
        "\n",
        "    conversation_chain = (ConversationalRetrievalChain.from_llm\n",
        "                          (llm=llama_llm,\n",
        "                           retriever=retriever,\n",
        "                           #condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
        "                           memory=memory,\n",
        "                           return_source_documents=True))\n",
        "    print(\"Conversational Chain created for the LLM using the vector store\")\n",
        "    return conversation_chain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yd62FlR595Cc",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "file = '/content/Student Allotment Letter harshal(1).pdf'\n",
        "content, metadata = prepare_docs(file)\n",
        "split_docs = get_text_chunks(content, metadata)\n",
        "vectordb = ingest_into_vectordb(split_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9ce9tsV7yoE",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "conversation_chain=get_conversation_chain(vectordb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tA-bhSGM846g",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# !pip install pickle5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IohRx8kS9HkP"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BA39CIsL8u2c"
      },
      "outputs": [],
      "source": [
        "with open('conversation_chain.pkl', 'wb') as f:\n",
        "  pickle.dump(conversation_chain, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpU2E-dr86sX",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "with open('conversation_chain.pkl', 'rb') as f:\n",
        "  conversation_chain = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Z98Y06c8MnL",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "user_question = \"what is the document about?\"\n",
        "response=conversation_chain({\"question\": user_question})\n",
        "print(\"Q: \",user_question)\n",
        "print(\"A: \",response['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9W9TIo2p8zQm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfw7XZSd_WvL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsB-zXR28zZH"
      },
      "outputs": [],
      "source": [
        "\n",
        "def greet(query,file):\n",
        "  content, metadata = prepare_docs(file)\n",
        "  split_docs = get_text_chunks(content, metadata)\n",
        "  vectordb = ingest_into_vectordb(split_docs)\n",
        "  conversation_chain=get_conversation_chain(vectordb)\n",
        "  user_question = query\n",
        "  response=conversation_chain({\"question\": user_question})\n",
        "  print(\"Q: \",user_question)\n",
        "  print(\"A: \",response['answer'])\n",
        "  return response['answer']\n",
        "\n",
        "    # vectorstore = generate_embeddings(file)\n",
        "    # ##retriever    # retriever = vectorstore.as_retriever(\n",
        "    # search_type=\"mmr\", #similarity\n",
        "    # search_kwargs={'k': 4})\n",
        "\n",
        "    # llm = HuggingFaceHub(\n",
        "    # repo_id=\"huggingfaceh4/zephyr-7b-alpha\",\n",
        "    # model_kwargs={\"temperature\": 0.5, \"max_length\": 64,\"max_new_tokens\":512},cache_dir = r\"C:\\Users\\janak.joshi\\Desktop\\gradio\\flagged\\model\")\n",
        "\n",
        "    # query = name\n",
        "\n",
        "    # prompt = f\"\"\"\n",
        "    # <|system|>\n",
        "    # You are an AI assistant that follows instruction extremely well.\n",
        "    # Please be truthful and give direct answers.\n",
        "    # also don't go outside the context, if there is something outside the context just say you don't know.\n",
        "    # </s>\n",
        "    # <|user|>\n",
        "    # {query}\n",
        "    # </s>\n",
        "    # <|assistant|>\n",
        "    # \"\"\"\n",
        "\n",
        "    # qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"refine\", retriever=retriever)\n",
        "\n",
        "    # response = qa.run(prompt)\n",
        "\n",
        "\n",
        "\n",
        "    # # print(page)\n",
        "\n",
        "    # return \"Hello \" + name + \"!!\" +\"and the answer is -- \" + \" \" +response\n",
        "\n",
        "iface = gr.Interface(greet, [\"text\",\"file\"], \"text\")\n",
        "iface.launch(share=True,debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pbGBV6usN3Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M9Ah2y6-N3O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UOgeNAubN3LR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qwo6MOm0N3H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pxhOPsQON3Fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IKoHhwodN3DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dIg8W3dVN3AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IG3X4nw1N296"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Fu4nV_7N27W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Below code For Advance Experiment"
      ],
      "metadata": {
        "id": "-U068ljsN24e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Oo7oSxBnXDL"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3uxjQ-5nAB2"
      },
      "outputs": [],
      "source": [
        "\n",
        "import streamlit as st\n",
        "\n",
        "# Define global variables to store processed document and vector database\n",
        "vectordb = None\n",
        "\n",
        "def process_document_once(file):\n",
        "    # Function to process the uploaded document and create a vector database only once\n",
        "    global vectordb\n",
        "    if vectordb is None:\n",
        "        content, metadata = prepare_docs(file)\n",
        "        split_docs = get_text_chunks(content, metadata)\n",
        "        vectordb = ingest_into_vectordb(split_docs)\n",
        "\n",
        "def get_answer(query):\n",
        "    # Function to retrieve answer from the vector database based on user's query\n",
        "    conversation_chain = get_conversation_chain(vectordb)\n",
        "    response = conversation_chain({\"question\": query})\n",
        "    return response['answer']\n",
        "\n",
        "def main():\n",
        "    st.title(\"Document Query Answering System\")\n",
        "\n",
        "    page = st.sidebar.radio(\"Navigation\", [\"Upload Document\", \"Ask Questions\"])\n",
        "\n",
        "    if page == \"Upload Document\":\n",
        "        st.header(\"Step 1: Upload Document\")\n",
        "        uploaded_file = st.file_uploader(\"Upload document\", type=[\"txt\", \"pdf\"])\n",
        "        if uploaded_file is not None:\n",
        "            process_document_once(uploaded_file)\n",
        "            st.success(\"Document uploaded successfully! Now you can proceed to ask questions.\")\n",
        "    elif page == \"Ask Questions\":\n",
        "        st.header(\"Step 2: Ask Questions\")\n",
        "        if vectordb is None:\n",
        "            st.warning(\"Please upload a document first.\")\n",
        "        else:\n",
        "            query = st.text_input(\"Enter your question:\")\n",
        "            if st.button(\"Get Answer\"):\n",
        "                answer = get_answer(query)\n",
        "                st.write(f\"For the question '{query}', the answer is: {answer}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdVuBcZOnhOY"
      },
      "outputs": [],
      "source": [
        "\n",
        "!streamlit run /usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOexJa7w_tP5"
      },
      "outputs": [],
      "source": [
        "\n",
        "#####3 final iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvaPkt7d_gOz"
      },
      "outputs": [],
      "source": [
        "# Iteration 2\n",
        "\n",
        "\n",
        "\n",
        "def greet(query,file):\n",
        "  content, metadata = prepare_docs(file)\n",
        "  split_docs = get_text_chunks(content, metadata)\n",
        "  vectordb = ingest_into_vectordb(split_docs)\n",
        "  conversation_chain=get_conversation_chain(vectordb)\n",
        "  user_question = query\n",
        "  response=conversation_chain({\"question\": user_question})\n",
        "  print(\"Q: \",user_question)\n",
        "  print(\"A: \",response['answer'])\n",
        "  return \"Hello, for question  \" + query + \"!!\" +\"and the answer is -- \" + \" \" +response['answer']\n",
        "\n",
        "    # vectorstore = generate_embeddings(file)\n",
        "    # ##retriever\n",
        "    # retriever = vectorstore.as_retriever(\n",
        "    # search_type=\"mmr\", #similarity\n",
        "    # search_kwargs={'k': 4})\n",
        "\n",
        "    # llm = HuggingFaceHub(\n",
        "    # repo_id=\"huggingfaceh4/zephyr-7b-alpha\",\n",
        "    # model_kwargs={\"temperature\": 0.5, \"max_length\": 64,\"max_new_tokens\":512},cache_dir = r\"C:\\Users\\janak.joshi\\Desktop\\gradio\\flagged\\model\")\n",
        "\n",
        "    # query = name\n",
        "\n",
        "    # prompt = f\"\"\"\n",
        "    # <|system|>\n",
        "    # You are an AI assistant that follows instruction extremely well.\n",
        "    # Please be truthful and give direct answers.\n",
        "    # also don't go outside the context, if there is something outside the context just say you don't know\n",
        "    # </s>\n",
        "    # <|user|>\n",
        "    # {query}\n",
        "    # </s>\n",
        "    # <|assistant|>\n",
        "    # \"\"\"\n",
        "\n",
        "    # qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"refine\", retriever=retriever)\n",
        "    # response = qa.run(prompt)\n",
        "    # # print(page)\n",
        "\n",
        "    # return \"Hello \" + name + \"!!\" +\"and the answer is -- \" + \" \" +response\n",
        "\n",
        "iface = gr.Interface(greet, [\"text\",\"file\"], \"text\")\n",
        "iface.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJCElfqeaODf"
      },
      "outputs": [],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTCbCOBmaMdR"
      },
      "outputs": [],
      "source": [
        "# importing the zipfile module\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# loading the temp.zip and creating a zip object\n",
        "with ZipFile(\"/content/djangoapp.zip\", 'r') as zObject:\n",
        "\n",
        "\t# Extracting all the members of the zip\n",
        "\t# into a specific location.\n",
        "\tzObject.extractall(\n",
        "\t\tpath=\"/content\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lt00xTw9aacn"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "\n",
        "def run_server():\n",
        "    !python main.py runserver\n",
        "\n",
        "# Start the Django development server in a separate thread\n",
        "server_thread = threading.Thread(target=run_server)\n",
        "server_thread.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEhLLrWifoiZ"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Function to setup ngrok tunnel\n",
        "def setup_ngrok():\n",
        "    ngrok.set_auth_token(\"2fOYbRXkWLrmz48Gu1gWqM7BgTC_2YR85vXtW65Zy1WP6ZE7J\")\n",
        "    public_url = ngrok.connect(addr=\"8000\", proto=\"http\")\n",
        "    print(\"Ngrok Tunnel URL:\", public_url)\n",
        "\n",
        "# Setup ngrok tunnel in a separate thread\n",
        "ngrok_thread = threading.Thread(target=setup_ngrok)\n",
        "ngrok_thread.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgHUWHjtjMre"
      },
      "outputs": [],
      "source": [
        "!python manage.py runserver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEI4yIfujPQy"
      },
      "outputs": [],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cG6hg-YDi4BU"
      },
      "outputs": [],
      "source": [
        "!python manage.py runserver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMMevtgEiRPL"
      },
      "outputs": [],
      "source": [
        "!ngrok http 8000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3NUeaWQim_1"
      },
      "outputs": [],
      "source": [
        "!ngrok config add-authtoken 2T6YXZSWoBe4TxXRUNrEcqUorwa_5SCckqKM6EwuaTsiUXbyc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Jbv2oqijt99"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcvN7cjGjt4w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zR4Nl9BpiphW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}